using System;
using System.Collections.Generic;
using LlmTornado.Chat;
using LlmTornado.Chat.Models;
using LlmTornado.ChatFunctions;
using LlmTornado.Code;
using LlmTornado.Common;
using Newtonsoft.Json;
using LlmTornado.Responses;

namespace LlmTornado.Responses;

/// <summary>
/// Result of <see cref="ResponseRequest"/>. Represents the full response object returned by the OpenAI Responses API.
/// </summary>
public class ResponseResult
{
    /// <summary>
    /// Unique identifier for this Response.
    /// </summary>
    [JsonProperty("id")]
    public string Id { get; set; } = string.Empty;

    /// <summary>
    /// The object type of this resource - always set to response.
    /// </summary>
    [JsonProperty("object")]
    public string Object { get; set; } = "response";

    /// <summary>
    /// The status of the response generation. One of completed, failed, in_progress, cancelled, queued, or incomplete.
    /// </summary>
    [JsonProperty("status")]
    public ResponseMessageStatuses Status { get; set; }

    /// <summary>
    /// The Unix timestamp (in seconds) for when the run step was created.
    /// </summary>
    [JsonProperty("created_at")]
    public int? CreatedAtUnixTimeSeconds { get; set; }

    /// <summary>
    /// An error object returned when the model fails to generate a Response.
    /// </summary>
    [JsonProperty("error")]
    public object? Error { get; set; }

    /// <summary>
    /// Details about why the response is incomplete.
    /// </summary>
    [JsonProperty("incomplete_details")]
    public IncompleteDetails? IncompleteDetails { get; set; }

    /// <summary>
    /// A system (or developer) message inserted into the model's context.
    /// </summary>
    [JsonProperty("instructions")]
    public string? Instructions { get; set; }

    /// <summary>
    /// An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
    /// </summary>
    [JsonProperty("max_output_tokens")]
    public int? MaxOutputTokens { get; set; }

    /// <summary>
    /// The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
    /// </summary>
    [JsonProperty("max_tool_calls")]
    public int? MaxToolCalls { get; set; }

    /// <summary>
    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.<br/>
    /// Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
    /// </summary>
    [JsonProperty("metadata")]
    public Dictionary<string, string>? Metadata { get; set; }

    /// <summary>
    /// Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
    /// </summary>
    [JsonProperty("model")]
    [JsonConverter(typeof(ChatModelJsonConverter))]
    public ChatModel? Model { get; set; }

    /// <summary>
    /// An array of content items generated by the model.
    /// </summary>
    [JsonProperty("output")]
    [JsonConverter(typeof(ResponseOutputItemListConverter))]
    public List<IResponseOutputItem>? Output { get; set; }

    /// <summary>
    /// Convenience property that contains the aggregated text output from all output_text items in the output array, if any are present.
    /// </summary>
    [JsonProperty("output_text")]
    public string? OutputText { get; set; }

    /// <summary>
    /// Whether to allow the model to run tool calls in parallel.
    /// </summary>
    [JsonProperty("parallel_tool_calls")]
    public bool? ParallelToolCalls { get; set; }

    /// <summary>
    /// The unique ID of the previous response to the model. Use this to create multi-turn conversations.
    /// </summary>
    [JsonProperty("previous_response_id")]
    public string? PreviousResponseId { get; set; }

    /// <summary>
    /// Reference to a prompt template and its variables.
    /// </summary>
    [JsonProperty("prompt")]
    public PromptConfiguration? Prompt { get; set; }

    /// <summary>
    /// Configuration options for reasoning models.
    /// </summary>
    [JsonProperty("reasoning")]
    public ReasoningConfiguration? Reasoning { get; set; }

    /// <summary>
    /// Whether to run the model response in the background.
    /// </summary>
    [JsonProperty("background")]
    public bool? Background { get; set; }

    /// <summary>
    /// Specifies the processing type used for serving the request.
    /// </summary>
    [JsonProperty("service_tier")]
    public string? ServiceTier { get; set; }

    /// <summary>
    /// Whether to store the generated model response for later retrieval via API. Defaults to true if null.
    /// </summary>
    [JsonProperty("store")]
    public bool? Store { get; set; }

    /// <summary>
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
    /// </summary>
    [JsonProperty("temperature")]
    public double? Temperature { get; set; }

    /// <summary>
    /// Configuration options for a text response from the model. Can be plain text or structured JSON data.
    /// </summary>
    [JsonProperty("text")]
    public TextConfiguration? Text { get; set; }

    /// <summary>
    /// How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how to specify which tools the model can call.
    /// </summary>
    [JsonProperty("tool_choice")]
    [JsonConverter(typeof(OutboundToolChoice.OutboundToolChoiceConverter))]
    public OutboundToolChoice? ToolChoice { get; set; }

    /// <summary>
    /// An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter.
    /// </summary>
    [JsonProperty("tools")]
    public List<ResponseTool>? Tools { get; set; }

    /// <summary>
    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
    /// </summary>
    [JsonProperty("top_logprobs")]
    public int? TopLogprobs { get; set; }

    /// <summary>
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.
    /// </summary>
    [JsonProperty("top_p")]
    public double? TopP { get; set; }

    /// <summary>
    /// The truncation strategy to use for the model response.
    /// </summary>
    [JsonProperty("truncation")]
    public ResponseTruncationStrategies? Truncation { get; set; }

    /// <summary>
    /// Represents token usage details including input tokens, output tokens, a breakdown of token categories, and the total tokens used.
    /// </summary>
    [JsonProperty("usage")]
    public ResponseUsage? Usage { get; set; }

    /// <summary>
    /// A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests and to help OpenAI detect and prevent abuse.
    /// </summary>
    [JsonProperty("user")]
    public string? User { get; set; }
}

/// <summary>
/// Details about why the response is incomplete.
/// </summary>
public class IncompleteDetails
{
    /// <summary>
    /// The reason why the response is incomplete. For example, "max_output_tokens" or "content_filter".
    /// </summary>
    [JsonProperty("reason")]
    public string? Reason { get; set; } // Could be an enum if you want to be stricter
}

/// <summary>
/// Output item for the response. Represents a single item in the output array, such as a message.
/// </summary>
public class ResponseOutputItem
{
    /// <summary>
    /// The type of the output item. For example, "message".
    /// </summary>
    [JsonProperty("type")]
    public string Type { get; set; } = string.Empty;

    /// <summary>
    /// The unique identifier of the output item.
    /// </summary>
    [JsonProperty("id")]
    public string Id { get; set; } = string.Empty;

    /// <summary>
    /// The status of the output item, if available.
    /// </summary>
    [JsonProperty("status")]
    public ResponseMessageStatuses? Status { get; set; }

    /// <summary>
    /// The role of the output item, if applicable (e.g., "assistant").
    /// </summary>
    [JsonProperty("role")]
    public ChatMessageRoles? Role { get; set; }

    /// <summary>
    /// The content of the output item, which may include text and annotations.
    /// </summary>
    [JsonProperty("content")]
    public List<OutputContent>? Content { get; set; }
}

/// <summary>
/// Output content for the response output item. Represents a single content part, such as text or annotations.
/// </summary>
public class OutputContent
{
    /// <summary>
    /// The type of the content part. For example, "output_text".
    /// </summary>
    [JsonProperty("type")]
    public string Type { get; set; } = string.Empty;

    /// <summary>
    /// The text content, if this part is text.
    /// </summary>
    [JsonProperty("text")]
    public string? Text { get; set; }

    /// <summary>
    /// Any annotations associated with this content part.
    /// </summary>
    [JsonProperty("annotations")]
    [JsonConverter(typeof(OutputContentAnnotationListConverter))]
    public List<IResponseOutputContentAnnotation>? Annotations { get; set; }
}

/// <summary>
/// Token usage details for a response.
/// </summary>
public class ResponseUsage
{
    /// <summary>
    /// The number of input tokens.
    /// </summary>
    [JsonProperty("input_tokens")]
    public int InputTokens { get; set; }

    /// <summary>
    /// Detailed breakdown of input tokens.
    /// </summary>
    [JsonProperty("input_tokens_details")]
    public ResponseInputTokensDetails InputTokensDetails { get; set; }

    /// <summary>
    /// The number of output tokens.
    /// </summary>
    [JsonProperty("output_tokens")]
    public int OutputTokens { get; set; }

    /// <summary>
    /// Detailed breakdown of output tokens.
    /// </summary>
    [JsonProperty("output_tokens_details")]
    public ResponseOutputTokensDetails OutputTokensDetails { get; set; }

    /// <summary>
    /// The total number of tokens used (input + output).
    /// </summary>
    [JsonProperty("total_tokens")]
    public int TotalTokens { get; set; }
}

/// <summary>
/// Breakdown of input tokens.
/// </summary>
public class ResponseInputTokensDetails
{
    /// <summary>
    /// Tokens retrieved from cache.
    /// </summary>
    [JsonProperty("cached_tokens")]
    public int CachedTokens { get; set; }
}

/// <summary>
/// Breakdown of output tokens.
/// </summary>
public class ResponseOutputTokensDetails
{
    /// <summary>
    /// Number of reasoning tokens.
    /// </summary>
    [JsonProperty("reasoning_tokens")]
    public int ReasoningTokens { get; set; }
}